
### 1. 선형회귀(Linear regression)
## 1.3 경사하강법(Gradient decent)

import numpy as np
import matplotlib.pyplot as plt


# Data
data = [[2, 81], [4, 93], [6, 91], [8, 97]]
x_data = [i[0] for i in data]
y_data = [i[1] for i in data]


# 데이터 변환하기(Converting data)
x = np.array(x_data)
y = np.array(y_data)
plt.scatter(x, y)


# 초기화(Initialization)
a = 0
b = 0


# 학습률(Learning rate)
RL = 0.05


# 반복 횟수(The number of repetitions)
epochs = 2000


# 경사하강법(Gradient decent)
for i in range(epochs):
    
    # 예측값의 일차함수 만들기(Creating a linear function of any predicted value)
    py = a * x + b
    
    # 에러(Error)
    er = py - y
    
    # 편미분(Partial differentiation)
    amse = np.mean(er * x)
    bmse = np.mean(er)
    
    # a와 b값 구하기(Getting values for a and b)
    a = a - RL * amse
    b = b - RL * bmse
    
    # epoch가 100번 반복될 때마다 아래 내용 출력(Every time the epoch is repeated 100 times, the contents below are printed out)
    if i % 200 == 0: 
        print("epoch = %.f, 기울기 = %.04f, 절편 = %.04f, 에러 = %.04f" % (i, a, b, er.mean()))

        
# 결과 보여주기(Showing the results)
plt.plot([min(x), max(x)], [min(py), max(py)], 'r')
plt.show
